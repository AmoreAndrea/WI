{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rBuVP98ceVIJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import re\n",
        "from statistics import mean, stdev"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning Section:\n",
        "\n",
        "Here we work on the csv file to prepare it for the following processing part. In particular we need to have the time in a correct format for extrapolating statistical features such as IAT (inter-arrival time)"
      ],
      "metadata": {
        "id": "0sZjR0wQmr0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to import all the csv files on colab in order to work with them (modifying or creating the dataset)\n",
        "\n",
        "traffic_files = ['zoom.csv',\n",
        "              'whatsapp.csv',\n",
        "              'wikipedia.csv',\n",
        "              'youtube.csv']\n",
        "\n",
        "for file_ in traffic_files:\n",
        "\n",
        "  # We read the csv file imported in a folder on colab\n",
        "  #df = pd.read_csv('/content/ds/' + file_)\n",
        "  # It is possible to do it without the folder by simply importing it directly on colab and then by doing\n",
        "  df = pd.read_csv(file_)\n",
        "\n",
        "  # we drop columns that we will not need\n",
        "  df = df.drop(columns=['frame.number'])\n",
        "\n",
        "  # Using regex we create a list with just the numbers\n",
        "  df['list'] = df['frame.time'].apply(lambda x: re.findall(r\"\\d+\", x))\n",
        "\n",
        "  # We create a second dataframe with the extracted time information\n",
        "  df2 = pd.DataFrame(df['list'].tolist(), columns=['day', 'year', 'hour', 'minute', 'second', 'microsecond'])\n",
        "\n",
        "  # We add the month\n",
        "  # Note: we cleared the month, which was written Aug, with the regex\n",
        "  df2['month'] = 8\n",
        "\n",
        "  # We create a datetime column on the original dataframe\n",
        "  df['datetime'] = pd.to_datetime(df2[['year', 'month', 'day', 'hour', 'minute', 'second', 'microsecond']])\n",
        "\n",
        "  # We drop columns that we will not need\n",
        "  df = df.drop(columns=['frame.time', 'list'])\n",
        "\n",
        "  # We drop rows without data.len parameters\n",
        "  df.dropna(inplace=True, subset = ['data.len'])\n",
        "\n",
        "  # We convert the datetime into the amount of seconds\n",
        "  df['timestamp'] = df['datetime'].apply(lambda x: (x - df['datetime'].iloc[0]).total_seconds())\n",
        "\n",
        "  # We remove the datetime column\n",
        "  df = df.drop(columns=['datetime'])"
      ],
      "metadata": {
        "id": "BqfWOom1mlIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Processing section:\n",
        "Here we create the dataset that will be used in the ML part. In this part we extrapolate the required features and then we create a complete dataset (uplink nad downlink) for each of the chosen steps (in seconds)"
      ],
      "metadata": {
        "id": "c_2-PBpsn2dY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # We create two different dataframes by filtering on mac addresses\n",
        "  # We define two dataframes\n",
        "  # One for uplink, denoted ul\n",
        "  # One for downlink, denoted dl\n",
        "\n",
        "  uplink_df = df[(df['wlan.sa']=='14:7d:da:93:75:71') & (df['wlan.da'] == '02:7d:60:f0:2d:64')]\n",
        "  downlink_df = df[(df['wlan.sa']=='02:7d:60:f0:2d:64') & (df['wlan.da'] == '14:7d:da:93:75:71')]\n",
        "\n",
        "  # We create the new DataFrame on which we will put the cleaned data\n",
        "  columns = ['avg datalen dl',\n",
        "             'std datalen dl',\n",
        "             'n_packets dl',\n",
        "             'avg iat dl',\n",
        "             'std iat dl',\n",
        "             'avg datalen ul',\n",
        "             'std datalen ul',\n",
        "             'n_packets ul',\n",
        "             'avg iat ul',\n",
        "             'std iat ul']\n",
        "\n",
        "  clean_dataset = pd.DataFrame(columns = columns)\n",
        "\n",
        "  # Index used in the for loop\n",
        "  index = 0\n",
        "\n",
        "  # We define parameters for the arange\n",
        "  max_time = downlink_df.iloc[-1]['timestamp']\n",
        "\n",
        "  # TIME INTERVAL ---> we decided to use 6 possible steps : 0.1s, 0.3s, 0.5s, 1s, 2s, 5s\n",
        "  step = 5\n",
        "\n",
        "  for i in np.arange(step, max_time, step):\n",
        "\n",
        "    # Create partition for downlink\n",
        "    dl = downlink_df[(downlink_df['timestamp'] >= i-step) & (downlink_df['timestamp'] < i)]\n",
        "    # Create partition for uplink\n",
        "    ul = uplink_df[(uplink_df['timestamp'] > i-step) & (uplink_df['timestamp'] < i)]\n",
        "\n",
        "    # Avg data_len downlink\n",
        "    dl_mean = dl['data.len'].mean()\n",
        "\n",
        "    # Std data_len downlink\n",
        "    dl_std = dl['data.len'].std()\n",
        "\n",
        "    # Number of packets downlink\n",
        "    dl_n = dl.shape[0]\n",
        "\n",
        "    # Computing interarrival times downlink\n",
        "    dl_iat = []\n",
        "    for j in range(dl['timestamp'].shape[0]-1):\n",
        "      previous = dl['timestamp'].iloc[j]\n",
        "      successive = dl['timestamp'].iloc[j+1]\n",
        "      dl_iat.append(successive-previous)\n",
        "\n",
        "    if len(dl_iat) != 0:\n",
        "      # Avg downlink interarrival time\n",
        "      dl_iat_mean = mean(dl_iat)\n",
        "\n",
        "      # Std downlink interarrival time\n",
        "      if len(dl_iat) >= 2:\n",
        "        dl_iat_std = stdev(dl_iat)\n",
        "      else:\n",
        "        # Stdev cannot be computed with less then two elements\n",
        "        dl_iat_std = -1\n",
        "\n",
        "    else:\n",
        "\n",
        "      # Mean cannot be computed with 0 elements\n",
        "      dl_iat_mean = 0\n",
        "\n",
        "\n",
        "    # Avg data_len uplink\n",
        "    ul_mean = ul['data.len'].mean()\n",
        "\n",
        "    # Std data_len uplink\n",
        "    ul_std = ul['data.len'].std()\n",
        "\n",
        "    # Number of packets uplink\n",
        "    ul_n = ul.shape[0]\n",
        "\n",
        "    # Computing interarrival times downlink\n",
        "    ul_iat = []\n",
        "    for j in range(ul['timestamp'].shape[0]-1):\n",
        "      previous = ul['timestamp'].iloc[j]\n",
        "      successive = ul['timestamp'].iloc[j+1]\n",
        "      ul_iat.append(successive-previous)\n",
        "\n",
        "\n",
        "    if len(ul_iat) != 0:\n",
        "\n",
        "      # Avg uplink interarrival time\n",
        "      # Mean need at least 1 element\n",
        "      ul_iat_mean = mean(ul_iat)\n",
        "\n",
        "      if len(ul_iat) >= 2:\n",
        "        # Std uplink interarrival time\n",
        "        # Std needs at least 2 elements\n",
        "        ul_iat_std = stdev(ul_iat)\n",
        "      else:\n",
        "        ul_iat_std = -1\n",
        "\n",
        "    else:\n",
        "\n",
        "      ul_iat_mean = 0\n",
        "\n",
        "\n",
        "    # Create the row\n",
        "    row = [dl_mean, dl_std, dl_n, dl_iat_mean, dl_iat_std, ul_mean, ul_std, ul_n, ul_iat_mean, ul_iat_std ]\n",
        "    # Add the new row\n",
        "    clean_dataset.loc[index] = row\n",
        "    index += 1\n",
        "\n",
        "  # We add the supervised label\n",
        "  name = re.findall(r'[a-z]*', file_)[0]\n",
        "  clean_dataset['supervised'] = name\n",
        "\n",
        "  # We save the dataset with a new name\n",
        "  clean_dataset.to_csv(name + '.csv')"
      ],
      "metadata": {
        "id": "MkIy7nuaezKV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we can create the complete dataset used in the 3) phase"
      ],
      "metadata": {
        "id": "1fH9hGlopL8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names = ['zoom', 'youtube', 'wikipedia', 'whatsapp']\n",
        "\n",
        "frames = [pd.read_csv(i+'.csv') for i in names]\n",
        "\n",
        "dataset = pd.concat(frames)\n",
        "\n",
        "dataset.to_csv('dataset_5.csv')"
      ],
      "metadata": {
        "id": "FJ0Rl26SgH2V"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}